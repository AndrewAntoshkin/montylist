/**
 * Init Processing V4 â€” Ñ PySceneDetect
 * 
 * ĞÑ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¾Ñ‚ v3:
 * - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ PySceneDetect Ğ²Ğ¼ĞµÑÑ‚Ğ¾ FFmpeg Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½
 * - AdaptiveDetector Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ğ»Ñ‹Ğ²Ğ¾Ğ²
 */

import { createServiceRoleClient } from '@/lib/supabase/server';
import { NextResponse } from 'next/server';
import type { NextRequest } from 'next/server';
import { createVideoChunks } from '@/lib/video-chunking';
import { downloadVideo, splitVideoIntoChunks, cleanupTempFiles } from '@/lib/video-splitter';
import { detectVideoFPS } from '@/lib/scene-detection';
import { 
  detectScenesWithPySceneDetect, 
  validatePySceneDetect,
  pyScenesToPlanBoundaries 
} from '@/lib/pyscenedetect';
import { mergeCreditsScenes, type MergedScene } from '@/lib/credits-detector';
import { smartMergeScenes } from '@/lib/smart-scene-merger';
import path from 'path';
import fs from 'fs';

// Feature flag for Face Recognition
const USE_FACE_RECOGNITION = process.env.USE_FACE_RECOGNITION === 'true';

// Dynamic import types for face clustering (only loaded when enabled)
type FaceCluster = {
  clusterId: string;
  appearances: number;
  firstSeen: number;
  lastSeen: number;
  characterName?: string | null;
  centroid?: number[];
};

// 5 minutes timeout
export const maxDuration = 300;
export const dynamic = 'force-dynamic';

export async function POST(request: NextRequest) {
  const tempFiles: string[] = [];
  
  // Save baseUrl early - ALWAYS use request.url for correct port
  const requestUrl = new URL(request.url);
  const savedBaseUrl = `${requestUrl.protocol}//${requestUrl.host}`;
  
  try {
    const { videoId, videoUrl, videoDuration, filmMetadata, scriptData } = await request.json();

    if (!videoId || !videoUrl || !videoDuration) {
      return NextResponse.json(
        { error: 'Missing required fields: videoId, videoUrl, videoDuration' },
        { status: 400 }
      );
    }

    console.log(`\n${'â•'.repeat(60)}`);
    console.log(`ğŸ¬ V4 INIT (PySceneDetect): Ğ’Ğ¸Ğ´ĞµĞ¾ ${videoId} (${videoDuration}s)`);
    console.log(`${'â•'.repeat(60)}`);
    
    // Log script data if present
    if (scriptData?.characters?.length > 0) {
      console.log(`ğŸ“‹ Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™: ${scriptData.characters.length} Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹`);
      const mainChars = scriptData.characters.filter((c: any) => c.dialogueCount >= 5);
      if (mainChars.length > 0) {
        console.log(`   ğŸŒŸ Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ: ${mainChars.map((c: any) => c.name).join(', ')}`);
      }
    }

    const supabase = createServiceRoleClient();

    // Atomic check-and-set to prevent duplicate initialization
    const { data: lockResult, error: lockError } = await supabase
      .from('videos')
      .update({ 
        status: 'processing',
        chunk_progress_json: { initializing: true, timestamp: new Date().toISOString(), processingVersion: 'v4' }
      })
      .eq('id', videoId)
      .eq('status', 'processing')
      .is('chunk_progress_json', null)
      .select('user_id');

    if (!lockResult || lockResult.length === 0) {
      console.log(`âš ï¸  Video ${videoId} is already being initialized`);
      
      const { data: existingVideo } = await supabase
        .from('videos')
        .select('chunk_progress_json')
        .eq('id', videoId)
        .single();
      
      const { data: existingSheet } = await supabase
        .from('montage_sheets')
        .select('id')
        .eq('video_id', videoId)
        .maybeSingle();
      
      return NextResponse.json({
        success: true,
        videoId,
        sheetId: existingSheet?.id,
        chunks: existingVideo?.chunk_progress_json?.chunks || [],
        totalChunks: existingVideo?.chunk_progress_json?.totalChunks || 0,
        message: 'Already initializing (duplicate blocked)',
      });
    }

    console.log(`ğŸ”’ Lock acquired for ${videoId}`);
    const video = lockResult[0];

    // Create chunks
    const chunks = createVideoChunks(videoDuration);
    console.log(`ğŸ“¦ Created ${chunks.length} chunks`);

    // Check/create montage sheet
    const { data: existingSheet } = await supabase
      .from('montage_sheets')
      .select('*')
      .eq('video_id', videoId)
      .maybeSingle();

    let sheet;
    if (existingSheet) {
      console.log(`âœ… Using existing sheet`);
      sheet = existingSheet;
    } else {
      const { data: newSheet, error: sheetError } = await supabase
        .from('montage_sheets')
        .insert({
          video_id: videoId,
          user_id: video.user_id,
          title: `ĞœĞ¾Ğ½Ñ‚Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ»Ğ¸ÑÑ‚ v4 PySceneDetect (${chunks.length} Ñ‡Ğ°ÑÑ‚ĞµĞ¹)`,
        })
        .select()
        .single();

      if (sheetError || !newSheet) {
        throw new Error('Failed to create montage sheet');
      }
      sheet = newSheet;
      console.log(`âœ… Created new sheet (v4 PySceneDetect)`);
    }

    // Initialize chunk progress with V4 marker
    const chunkProgress: any = {
      totalChunks: chunks.length,
      completedChunks: 0,
      currentChunk: 0,
      sheetId: sheet.id,
      processingVersion: 'v4', // <-- ĞœĞĞ ĞšĞ•Ğ  V4
      sceneDetector: 'pyscenedetect', // <-- ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸
      scriptData: scriptData || null,
      chunks: chunks.map(chunk => ({
        index: chunk.chunkIndex,
        status: 'pending' as const,
        startTimecode: chunk.startTimecode,
        endTimecode: chunk.endTimecode,
        storageUrl: null as string | null,
      })),
    };

    // Download video
    const tempDir = '/tmp/video-chunks-v4';
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }
    
    const originalVideoPath = path.join(tempDir, `original_${videoId}.mp4`);
    console.log(`ğŸ“¥ Downloading video...`);
    await downloadVideo(videoUrl, originalVideoPath);
    tempFiles.push(originalVideoPath);
    
    // Detect FPS
    let videoFPS = 24;
    try {
      videoFPS = await detectVideoFPS(originalVideoPath);
    } catch (e) {
      console.warn(`âš ï¸ Could not detect FPS, using default ${videoFPS}`);
    }
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ğŸ­ FACE RECOGNITION (optional)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let faceClusters: FaceCluster[] = [];
    
    if (USE_FACE_RECOGNITION) {
      console.log(`\nğŸ­ FACE RECOGNITION enabled - starting face clustering...`);
      
      try {
        // Use Worker mode to bypass Turbopack TensorFlow.js compatibility issues
        // Worker runs in separate Node.js process without bundler interference
        const { clusterFacesInVideoWorker } = await import('@/lib/face-clustering');
        
        faceClusters = await clusterFacesInVideoWorker(originalVideoPath, {
          frameInterval: 5,        // ĞšĞ°Ğ¶Ğ´Ñ‹Ğµ 5 ÑĞµĞºÑƒĞ½Ğ´
          distanceThreshold: 0.5,  // ĞŸĞ¾Ñ€Ğ¾Ğ³ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸
          minAppearances: 5,       // ĞœĞ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 5 Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹
        });
        
        console.log(`âœ… Found ${faceClusters.length} unique characters`);
        
        // Worker handles cleanup internally
        
      } catch (faceError) {
        console.error(`âŒ Face recognition failed:`, faceError);
        console.log(`   Continuing without face recognition...`);
      }
    } else {
      console.log(`â„¹ï¸  Face Recognition disabled (set USE_FACE_RECOGNITION=true to enable)`);
    }
    
    // Run PySceneDetect scene detection
    let detectedScenes: Array<{ timecode: string; timestamp: number }> = [];
    
    try {
      const isPySceneDetectAvailable = await validatePySceneDetect();
      
      if (isPySceneDetectAvailable) {
        console.log(`\nğŸ” PySceneDetect scene detection (AdaptiveDetector)...`);
        
        const rawScenes = await detectScenesWithPySceneDetect(originalVideoPath, { 
          fps: videoFPS,
          adaptiveThreshold: 1.8,  // â¬‡ï¸ ĞŸĞĞĞ˜Ğ–Ğ•Ğ Ğ´Ğ¾ 1.8 â€” Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ ÑÑ†ĞµĞ½ (Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ²!)
          minSceneDuration: 0.25,  // â¬‡ï¸ ĞŸĞĞĞ˜Ğ–Ğ•Ğ Ğ´Ğ¾ 0.25 ÑĞµĞº â€” Ğ»Ğ¾Ğ²Ğ¸Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹
          maxScenes: 5000,
        });
        
        // ğŸ”€ SMART MERGING â€” ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹
        console.log(`\nğŸ”€ Applying smart scene merging...`);
        const smartMerged = smartMergeScenes(rawScenes, {
          ultraShortThreshold: 0.3,  // <0.3 ÑĞµĞº â€” Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚ (Ğ²ÑĞ¿Ñ‹ÑˆĞºĞ°)
          shortThreshold: 0.8,       // ĞĞµ Ñ‚Ñ€Ğ¾Ğ³Ğ°ĞµĞ¼ ÑÑ†ĞµĞ½Ñ‹ >0.8 ÑĞµĞº
        });
        
        detectedScenes = smartMerged.map(s => ({
          timecode: s.timecode,
          timestamp: s.timestamp,
        }));
        
        console.log(`âœ… PySceneDetect: ${detectedScenes.length} scene changes`);
        
        // Add closing scene if needed
        const lastSceneTime = detectedScenes[detectedScenes.length - 1]?.timestamp || 0;
        if (videoDuration - lastSceneTime > 2.0) {
          const totalFrames = Math.round(videoDuration * videoFPS);
          const frames = ((totalFrames % videoFPS) + videoFPS) % videoFPS;
          const totalSeconds = Math.floor(totalFrames / videoFPS);
          const secs = totalSeconds % 60;
          const totalMinutes = Math.floor(totalSeconds / 60);
          const mins = totalMinutes % 60;
          const hours = Math.floor(totalMinutes / 60);
          const finalTimecode = `${hours.toString().padStart(2, '0')}:${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}:${frames.toString().padStart(2, '0')}`;
          detectedScenes.push({ timecode: finalTimecode, timestamp: videoDuration });
          console.log(`ğŸ“ Added closing at ${finalTimecode}`);
        }
        
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // ĞĞ‘ĞªĞ•Ğ”Ğ˜ĞĞ¯Ğ•Ğœ Ğ·Ğ°ÑÑ‚Ğ°Ğ²ĞºÑƒ Ğ”Ğ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ² Gemini
        // 
        // Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ»Ğ¸ÑÑ‚:
        // - ĞŸĞ»Ğ°Ğ½ 1: Ğ›Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿ (00:00:00:00 - 00:00:04:09) = ~4 ÑĞµĞº
        // - ĞŸĞ»Ğ°Ğ½ 2: Ğ—Ğ°ÑÑ‚Ğ°Ğ²ĞºĞ° (00:00:04:09 - 00:01:06:13) = ~62 ÑĞµĞº = ĞĞ”Ğ˜Ğ Ğ¿Ğ»Ğ°Ğ½!
        // 
        // Ğ‘ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ: PySceneDetect Ğ½Ğ°Ñ€ĞµĞ·Ğ°ĞµÑ‚ Ğ·Ğ°ÑÑ‚Ğ°Ğ²ĞºÑƒ Ğ½Ğ° 20-30 Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ²,
        // Ñ‡Ñ‚Ğ¾ Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ñƒ.
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        console.log(`\nğŸ¬ Merging credits scenes (logo + opening credits)...`);
        const mergedScenes = mergeCreditsScenes(detectedScenes, videoDuration, videoFPS, {
          skipCreditsMerging: false, // Ğ’ĞšĞ›Ğ®Ğ§Ğ•ĞĞ: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ·Ğ°ÑÑ‚Ğ°Ğ²ĞºÑƒ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ»Ğ°Ğ½
        });
        chunkProgress.mergedScenes = mergedScenes;
        console.log(`ğŸ“Š Credits merged: ${detectedScenes.length} raw â†’ ${mergedScenes.length} plans`);
        
      } else {
        console.warn(`âš ï¸ PySceneDetect not available, falling back to FFmpeg`);
        
        // Fallback to FFmpeg
        const { detectScenes, validateFFmpeg } = await import('@/lib/scene-detection');
        const isFFmpegAvailable = await validateFFmpeg();
        
        if (isFFmpegAvailable) {
          const rawScenes = await detectScenes(originalVideoPath, { 
            threshold: 0.05,
            fps: videoFPS,
          });
          
          detectedScenes = rawScenes.map(s => ({
            timecode: s.timecode,
            timestamp: s.timestamp,
          }));
          
          if (detectedScenes.length === 0 || detectedScenes[0].timestamp > 0.5) {
            detectedScenes.unshift({ timecode: '00:00:00:00', timestamp: 0 });
          }
          
          const ffmpegMergedScenes = mergeCreditsScenes(detectedScenes, videoDuration, videoFPS);
          chunkProgress.mergedScenes = ffmpegMergedScenes;
          chunkProgress.sceneDetector = 'ffmpeg-fallback';
        }
      }
    } catch (e) {
      console.error(`âŒ Scene detection failed:`, e);
    }
    
    chunkProgress.detectedScenes = detectedScenes;
    chunkProgress.videoFPS = videoFPS;
    
    // Save face clusters (serialized for JSON storage)
    if (faceClusters.length > 0) {
      chunkProgress.faceClusters = faceClusters.map(cluster => ({
        clusterId: cluster.clusterId,
        appearances: cluster.appearances,
        firstSeen: cluster.firstSeen,
        lastSeen: cluster.lastSeen,
        characterName: cluster.characterName || null,
        // Store centroid as array for JSON serialization
        centroid: Array.from(cluster.centroid),
        // Store face timestamps for scene matching
        faceTimestamps: cluster.faces.map(f => f.timestamp),
      }));
      chunkProgress.useFaceRecognition = true;
      console.log(`ğŸ“Š Saved ${faceClusters.length} face clusters to progress`);
    }
    
    // Split video into chunks
    console.log(`\nâœ‚ï¸  Splitting into ${chunks.length} chunks...`);
    const chunkFiles = await splitVideoIntoChunks(
      originalVideoPath,
      chunks.map(c => ({
        chunkIndex: c.chunkIndex,
        startTime: c.startTime,
        endTime: c.endTime,
      })),
      tempDir
    );
    
    tempFiles.push(...chunkFiles.map(c => c.localPath));
    
    // Upload chunks in parallel (2 at a time - Supabase rate limit friendly)
    console.log(`\nâ˜ï¸  Uploading ${chunkFiles.length} chunks (parallel x2)...`);
    
    const PARALLEL_UPLOADS = 2;
    const uploadChunk = async (chunkFile: { chunkIndex: number; localPath: string }) => {
      const chunkStoragePath = `${video.user_id}/chunks-v4/chunk_${chunkFile.chunkIndex}_${Date.now()}.mp4`;
      
      const stats = fs.statSync(chunkFile.localPath);
      const sizeMB = (stats.size / (1024 * 1024)).toFixed(2);
      console.log(`ğŸ“¦ Chunk ${chunkFile.chunkIndex}: ${sizeMB} MB`);

      // Upload with retry
      const MAX_ATTEMPTS = 3;
      for (let attempt = 1; attempt <= MAX_ATTEMPTS; attempt++) {
        try {
          const fileBuffer = fs.readFileSync(chunkFile.localPath);
          
          const { error: uploadError } = await supabase.storage
            .from('videos')
            .upload(chunkStoragePath, fileBuffer, {
              contentType: 'video/mp4',
              upsert: true,
            });

          if (!uploadError) {
            console.log(`âœ… Chunk ${chunkFile.chunkIndex} uploaded (${sizeMB} MB)`);
            break;
          }

          if (uploadError.message?.includes('already exists')) {
            console.log(`âœ… Chunk ${chunkFile.chunkIndex} exists`);
            break;
          }

          console.log(`âš ï¸ Chunk ${chunkFile.chunkIndex} attempt ${attempt} failed: ${uploadError.message}`);
          if (attempt === MAX_ATTEMPTS) {
            throw new Error(`Upload failed: ${uploadError.message}`);
          }
        } catch (err) {
          console.log(`âŒ Chunk ${chunkFile.chunkIndex} error attempt ${attempt}: ${err}`);
          if (attempt === MAX_ATTEMPTS) throw err;
          await new Promise(res => setTimeout(res, 2000 * attempt));
        }
      }

      // Get signed URL directly (faster than checking public URL)
      const { data: signedUrlData } = await supabase.storage
        .from('videos')
        .createSignedUrl(chunkStoragePath, 60 * 60 * 24 * 7);
      
      chunkProgress.chunks[chunkFile.chunkIndex].storageUrl = signedUrlData?.signedUrl || '';
    };

    // Process in batches of PARALLEL_UPLOADS
    for (let i = 0; i < chunkFiles.length; i += PARALLEL_UPLOADS) {
      const batch = chunkFiles.slice(i, i + PARALLEL_UPLOADS);
      console.log(`ğŸ“¤ Batch ${Math.floor(i/PARALLEL_UPLOADS) + 1}/${Math.ceil(chunkFiles.length/PARALLEL_UPLOADS)}: chunks ${batch.map(c => c.chunkIndex).join(', ')}`);
      await Promise.all(batch.map(uploadChunk));
    }

    // Save progress
    await supabase
      .from('videos')
      .update({ chunk_progress_json: chunkProgress })
      .eq('id', videoId);

    // Cleanup
    console.log('ğŸ§¹ Cleaning up temp files...');
    cleanupTempFiles(tempFiles);

    console.log(`\nâœ… V4 INIT COMPLETE. Ready to process ${chunks.length} chunks with PySceneDetect data`);

    // Trigger background processing (V4 endpoint!)
    console.log(`ğŸš€ Triggering V4 processing for ${videoId} via ${savedBaseUrl}`);
    
    fetch(`${savedBaseUrl}/api/process-all-chunks-v4`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ videoId }),
    }).catch(err => {
      console.error(`âŒ Failed to trigger V4 processing:`, err);
    });

    return NextResponse.json({
      success: true,
      videoId,
      sheetId: sheet.id,
      processingVersion: 'v4',
      sceneDetector: chunkProgress.sceneDetector,
      totalScenes: detectedScenes.length,
      faceRecognition: {
        enabled: USE_FACE_RECOGNITION,
        clustersFound: faceClusters.length,
      },
      chunks: chunkProgress.chunks.map((c: any) => ({
        index: c.index,
        startTimecode: c.startTimecode,
        endTimecode: c.endTimecode,
        storageUrl: c.storageUrl,
        status: c.status,
      })),
      totalChunks: chunks.length,
    });

  } catch (error) {
    console.error('V4 Init Error:', error);
    
    if (tempFiles.length > 0) {
      try {
        cleanupTempFiles(tempFiles);
      } catch {}
    }

    return NextResponse.json(
      { error: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}


