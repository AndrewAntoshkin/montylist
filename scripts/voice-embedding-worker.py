#!/usr/bin/env python3
"""
Voice Embedding Worker ‚Äî –°–æ–∑–¥–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –æ—Ç–ø–µ—á–∞—Ç–∫–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç resemblyzer –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è voice embeddings –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤.

–ê–ª–≥–æ—Ä–∏—Ç–º:
1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ speaker ID –∏–∑ –≤–∏–¥–µ–æ
2. –°–æ–∑–¥–∞—ë–º embeddings –¥–ª—è –∫–∞–∂–¥–æ–≥–æ speaker
3. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –≥–æ–ª–æ—Å–∞–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
4. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Ç–æ—á–Ω—ë–Ω–Ω—ã–π speaker‚Üícharacter mapping

@author AI Assistant
@version 1.0
"""

import sys
import os
import json
import subprocess
import tempfile
from pathlib import Path

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
try:
    import numpy as np
    from resemblyzer import VoiceEncoder, preprocess_wav
except ImportError as e:
    print(f"‚ùå Missing dependency: {e}", file=sys.stderr)
    print("Run: pip3 install resemblyzer numpy", file=sys.stderr)
    sys.exit(1)


def extract_audio_segment(video_path: str, start_ms: int, end_ms: int, output_path: str) -> bool:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ-—Å–µ–≥–º–µ–Ω—Ç –∏–∑ –≤–∏–¥–µ–æ"""
    start_sec = start_ms / 1000
    duration = (end_ms - start_ms) / 1000
    
    cmd = [
        'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
        '-ss', str(start_sec),
        '-i', video_path,
        '-t', str(duration),
        '-vn',  # –ë–µ–∑ –≤–∏–¥–µ–æ
        '-acodec', 'pcm_s16le',
        '-ar', '16000',  # 16kHz –¥–ª—è resemblyzer
        '-ac', '1',  # –ú–æ–Ω–æ
        output_path
    ]
    
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        return os.path.exists(output_path) and os.path.getsize(output_path) > 1000
    except subprocess.CalledProcessError:
        return False


def create_speaker_embeddings(video_path: str, diarization_words: list) -> dict:
    """
    –°–æ–∑–¥–∞—ë—Ç voice embeddings –¥–ª—è –∫–∞–∂–¥–æ–≥–æ speaker ID
    
    Args:
        video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—É
        diarization_words: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ —Å speaker ID –∏ —Ç–∞–π–º–∫–æ–¥–∞–º–∏
        
    Returns:
        Dict[speaker_id, embedding (list of floats)]
    """
    print("üé§ Creating voice embeddings...")
    
    encoder = VoiceEncoder()
    speaker_embeddings = {}
    speaker_segments = {}
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –ø–æ speaker ID
    for word in diarization_words:
        speaker = word.get('speaker', 'UNKNOWN')
        if speaker == 'UNKNOWN':
            continue
            
        if speaker not in speaker_segments:
            speaker_segments[speaker] = []
        
        speaker_segments[speaker].append({
            'start': word.get('startMs', word.get('start', 0)),
            'end': word.get('endMs', word.get('end', 0)),
            'text': word.get('text', word.get('word', ''))
        })
    
    print(f"   Found {len(speaker_segments)} speakers")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        for speaker_id, segments in speaker_segments.items():
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 10 —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ embedding)
            sample_segments = segments[:10]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–ª–∏–∑–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            merged_segments = []
            current = None
            
            for seg in sorted(sample_segments, key=lambda x: x['start']):
                if current is None:
                    current = seg.copy()
                elif seg['start'] - current['end'] < 500:  # < 500ms gap
                    current['end'] = seg['end']
                    current['text'] += ' ' + seg['text']
                else:
                    if current['end'] - current['start'] > 500:  # > 500ms
                        merged_segments.append(current)
                    current = seg.copy()
            
            if current and current['end'] - current['start'] > 500:
                merged_segments.append(current)
            
            if not merged_segments:
                print(f"   ‚ö†Ô∏è  Speaker {speaker_id}: no valid segments")
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ –∏ —Å–æ–∑–¥–∞—ë–º embeddings
            embeddings = []
            
            for i, seg in enumerate(merged_segments[:5]):  # –ú–∞–∫—Å 5 —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                audio_path = os.path.join(temp_dir, f"{speaker_id}_{i}.wav")
                
                if extract_audio_segment(video_path, seg['start'], seg['end'], audio_path):
                    try:
                        wav = preprocess_wav(audio_path)
                        if len(wav) > 0:
                            embedding = encoder.embed_utterance(wav)
                            embeddings.append(embedding)
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Error processing {speaker_id}_{i}: {e}")
            
            if embeddings:
                # –£—Å—Ä–µ–¥–Ω—è–µ–º embeddings –¥–ª—è —ç—Ç–æ–≥–æ speaker
                avg_embedding = np.mean(embeddings, axis=0)
                speaker_embeddings[speaker_id] = avg_embedding.tolist()
                print(f"   ‚úÖ Speaker {speaker_id}: {len(embeddings)} segments ‚Üí embedding created")
            else:
                print(f"   ‚ö†Ô∏è  Speaker {speaker_id}: failed to create embedding")
    
    return speaker_embeddings


def compare_speakers(embeddings1: dict, embeddings2: dict) -> dict:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ speaker embeddings
    
    Returns:
        Dict —Å similarity scores –º–µ–∂–¥—É speakers
    """
    from numpy.linalg import norm
    
    similarities = {}
    
    for speaker1, emb1 in embeddings1.items():
        emb1 = np.array(emb1)
        similarities[speaker1] = {}
        
        for speaker2, emb2 in embeddings2.items():
            emb2 = np.array(emb2)
            # Cosine similarity
            similarity = np.dot(emb1, emb2) / (norm(emb1) * norm(emb2))
            similarities[speaker1][speaker2] = float(similarity)
    
    return similarities


def find_best_matches(similarities: dict, threshold: float = 0.75) -> dict:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–µ–∂–¥—É speakers
    
    Args:
        similarities: –ú–∞—Ç—Ä–∏—Ü–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        
    Returns:
        Dict[speaker_id, (matched_character, confidence)]
    """
    matches = {}
    
    for speaker, scores in similarities.items():
        best_match = max(scores.items(), key=lambda x: x[1])
        if best_match[1] >= threshold:
            matches[speaker] = {
                'character': best_match[0],
                'confidence': best_match[1],
                'method': 'voice_embedding'
            }
    
    return matches


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è worker'–∞"""
    if len(sys.argv) < 3:
        print("Usage: voice-embedding-worker.py <video_path> <diarization_json>", file=sys.stderr)
        sys.exit(1)
    
    video_path = sys.argv[1]
    diarization_json = sys.argv[2]
    reference_json = sys.argv[3] if len(sys.argv) > 3 else None
    
    print("\n" + "‚ïê" * 60)
    print("üé§ VOICE EMBEDDING WORKER")
    print("‚ïê" * 60)
    print(f"   Video: {os.path.basename(video_path)}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
    with open(diarization_json, 'r') as f:
        diarization_words = json.load(f)
    
    print(f"   Words: {len(diarization_words)}")
    
    # –°–æ–∑–¥–∞—ë–º embeddings
    speaker_embeddings = create_speaker_embeddings(video_path, diarization_words)
    
    result = {
        'embeddings': speaker_embeddings,
        'speaker_count': len(speaker_embeddings)
    }
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–∞ ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
    if reference_json and os.path.exists(reference_json):
        print("\nüìä Comparing with reference voices...")
        with open(reference_json, 'r') as f:
            reference_embeddings = json.load(f)
        
        similarities = compare_speakers(speaker_embeddings, reference_embeddings)
        matches = find_best_matches(similarities)
        
        result['similarities'] = similarities
        result['matches'] = matches
        
        print("\n   Best matches:")
        for speaker, match in matches.items():
            print(f"      {speaker} ‚Üí {match['character']} ({match['confidence']:.0%})")
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print("\n" + "‚ïê" * 60)
    print("üìä VOICE EMBEDDING COMPLETE")
    print("‚ïê" * 60)
    print(f"   Speakers processed: {len(speaker_embeddings)}")
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á–µ—Ä–µ–∑ stdout (JSON)
    print("\n__RESULT_JSON__")
    print(json.dumps(result))


if __name__ == '__main__':
    main()
